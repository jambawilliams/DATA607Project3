---
title: 'DATA 607: Project 3'
author: "Michael Munguia, Keshaw Sahay, James Williams & Chunsan Yip"
date: "3/22/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(stringr)
library(rvest)
library(DT)
library(kableExtra)
library(reshape)
library(grid)
library(gridExtra)
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
nyc_jobs <- "https://raw.githubusercontent.com/mijomu/DATA-607/master/Project%203/NYC_Jobs.csv"
vs_file <- "https://raw.githubusercontent.com/ferrysany/CUNY607P3/master/valuedSkills.csv"
surverydata_link<-"https://raw.githubusercontent.com/keshaws/CUNY_MSDS_2020/master/DATA607/multipleChoiceResponses.csv"
```

# Introduction

The project goal was to gather and explore data revolving around the necessary job skills for a data scientist in today's job market. As a group, we discussed potential avenues for data collection via slack and based on availabilities and time constraints, executed the most readily accessible strategies. Unlike a real-world work scenario, we do not have most of our waking hours to dedicate to this, so we had to operate quickly and efficiently to deliver.

# Overall Process

There was no single process for gathering the data, but we focused on three main sources of data: job postings from NYC's various government agencies, articles written about the important skills needed by a data scientist and survey responses regarding data science working skills. The process for gathering data was two fold: mining data files for the job posts and survey results along with manual collection for the articles.

# Data gathering & exploration

## NYC Open Data: Job Postings

### Function writing

Creating a few custom functions, the process for collecting and processing data from job descriptions posted onto the city's job post mechanism boiled down to reading a CSV and tokenizing the two columns dictating required and preferred skills such that we break each text document down into its constituent words. The text needed to be cleaned to standardize the words across descriptions such that artificially distinct words were not created as a result of things like special characters, punctuation, whitespace or capitalization. The cleaning process is managed by the `clean_description` function and tokenization was handled by the `tidytext::unnest_tokens` function. Once cleaned and tokenized, the text data was passed to the `word_frequency` function to remove any stop words, including a special list created by iteratively evaluating the results, and ultimately generate counts for each word in the overall corpus of description texts.

```{r word-mining, eval=TRUE}
library(tidytext)
clean_description <- function(text) {
  text <- text %>% 
    iconv("", "ASCII", "byte") %>% 
    str_remove_all("<.+>|\t|\\d|[:punct:]") %>% 
    str_trim() %>% 
    str_to_lower()
  
  return(text)
}
word_frequency <- function(df, addl_stopwords = NA) {
  stop_words <- get_stopwords()
  if (is.character(addl_stopwords)) {
    stop_words <- bind_rows(stop_words,
                            tibble(word = addl_stopwords,
                                   lexicon = rep("user defined",
                                                 length(addl_stopwords)
                                   )
                            )
    )
  }
      
  df %>% 
    anti_join(stop_words, by = "word") %>% 
    count(word, sort = TRUE)
}
unnest_wrapper <- function(df, target_var, word_var = "word") {
  df %>%
    unnest_tokens_(word_var, target_var) %>%
    select(title_id, category_id, word)
}
word_plot <- function(df, addl_stopwords = NA, top_n = 10,
                      title_lab = "Job Posting",
                      x_lab = "Words in Text", y_lab = "Frequency",
                      fill_color = "gray") {
  top_df <- word_frequency(df, addl_stopwords) %>% head(top_n)
  word_order <- top_df$word
  top_df <- top_df %>% mutate_at("word", factor, ordered = TRUE, levels = word_order)
  
  result_plot <- ggplot(top_df, aes(word, n)) +
    geom_col(fill = fill_color) +
    labs(title = title_lab, x = x_lab, y = y_lab)
  
  if (top_n < 11) {
    return(result_plot)
  } else {
    return(result_plot + coord_flip())
  }
  
}
# These are additional stopwords that appear to be noise in the greater scheme
# of what we're trying to do (i.e. extract meaningful words from job postings)
jobpost_stopwords <- c("experience", "years", "accredited", "college", "degree",
                       "field", "equivalent", "appropriate", "education",
                       "specialization", "described", "year", "ability",
                       "skills", "data", "strong", "work", "working", "well",
                       "must", "candidate", "candidates", "university", "least",
                       "community", "fulltime", "school", "andor", "one", "two",
                       "three", "four", "five", "six", "seven", "eight", "nine",
                       "ten", "high", "level", "area", "related", "however",
                       "satisfactory", "assignment", "public", "duties", "social",
                       "activities", "centered", "responsible", "diploma", "period",
                       "probationary", "substituted", "accrediting", "graduation",
                       "recognized", "approved", "fouryear", "note", "required",
                       "capacity", "department", "development", "organization",
                       "states", "credits", "ii", "appointed", "appointments",
                       "program", "position", "can", "combination", "may",
                       "subject", "including", "semester", "appointment", "basis",
                       "graduate", "acquired", "made", "major", "requirements",
                       "use", "agencies", "areas", "bodies", "chea", "council",
                       "estate", "following", "hire", "june", "meeting", "past",
                       "real", "promotion", "primarily", "plus", "us", "using",
                       "user", "valid", "additional", "august", "b", "c", "base",
                       "eligible", "employee", "employment", "end", "fields",
                       "fifteen", "general", "held", "paid", "possess",
                       "prospective", "qualification", "recent", "winterspring",
                       "within", "without", "yearforyear", "oror", "post", "agency",
                       "city", "minimum", "months", "new", "york", "large", "highly",
                       "desired", "able", "interest", "part", "etc", "strongly",
                       "time", "prefer", "preference", "nyc", "environment",
                       "excellent", "familiarity", "demonstrated", "computer",
                       "administration", "administrative", "tasks", "student")
```

### Data wrangling

After loading the `tidytext` library and defining my custom functions, the data is loaded and run through the process described above. Additionally, unique identifiers are created for both the job titles and categories listed in the job posts.

```{r, eval=TRUE, message=FALSE}
jobs <- read_csv(nyc_jobs) %>%
  select("title" = `Business Title`, "category" = `Job Category`,
         "requirements" = `Minimum Qual Requirements`,
         "preferences" = `Preferred Skills`)
data_jobs <- jobs %>%
  filter(str_detect(title, regex("data",ignore_case = TRUE)) &
           str_detect(title, regex("analyst|scientist|engineer", ignore_case = TRUE))) %>%
  mutate_at(c("requirements", "preferences"), clean_description)
title_ids <- data_jobs %>% group_indices(title)
category_ids <- data_jobs %>% group_indices(category)
data_jobs$title_id <- title_ids
data_jobs$category_id <- category_ids
```

### Putting it all together

With unique identifiers in hand, separate dataframes are established for later normalized SQL table creation. In essence: `title_table` for job titles, `category_table` for departmental categories and `job_requirements` for required skills and `job_preferences` for preferred skills. The primary key linking these all will be the unique `title_id` and/or `category_id`. This leaves room open later on to further standardize the job titles such that departmental information could be extracted from the titles themselves if we eventually wanted to dissect job skills for data analysts versus data scientists, senior positions versus junior positions, etc.

``` {r, eval=TRUE}
(title_table <- data_jobs %>% distinct(title_id, title))
(category_table <- data_jobs %>% distinct(category_id, category))
data_jobs <- data_jobs %>% distinct(title_id, category_id, requirements, preferences)
(job_requirements <- data_jobs %>% unnest_wrapper("requirements"))
(job_preferences <- data_jobs %>% unnest_wrapper("preferences"))
```

### Database interlude I

Writing the tables above as CSV Files, normalized tables were created and populated via the PGAdmin 4 GUI. Alternatively, a backup file is included in ![this repository](https://github.com/mijomu/DATA-607/tree/master/Project%203). To load the data back into R, a process like below could be followed. For reproducibility, I maintained the dataframes created thus far, but if one were to instantiate the database via the backup file and load accordingly the results will be the same.:

```{r, echo=TRUE, eval=FALSE}
library(DBI)
library(odbc)
connection <- dbConnect(RPostgres::Postgres(), dbname = params$db_name,
                        host = params$host, user = params$user, password = params$pwd)
read_db_tbl <- function(conx, select_statement) {
  as.tbl(dbGetQuery(conn = conx, statement = select_statement))
}
title_table <- read_db_tbl(connection, "SELECT * FROM title_table")
category_table <- read_db_tbl(connection, "SELECT * FROM category_table")
job_requirements <- read_db_tbl(connection, "SELECT * FROM job_requirements")
job_preferences <- read_db_tbl(connection, "SELECT * FROM job_preferences")
```


### Analysis

The top twenty most frequent words that appear in required job skills are below. More work is required here to adequately parse descriptions, but we can get a handle on the fact that most required skills revolve around having a more scientific background - words like research, biological, environmental and both masters and baccalaureate round out the top ten along with words like management and professional. We can glean from this that, at least as far as the city of New York is concerned, they are looking to hire data specialists with a higher degree of education than not. A research background in one of the sciences may give a potential applicant an edge over the competition.

``` {r, eval=TRUE}
word_plot(job_requirements, jobpost_stopwords, 20, title = "Required Job Skills", fill_color = "gray")
```

Likewise, we can see preferred job skills below. It is interesting to me that no language indicating any specific tools appears above, but we do see plenty of them below. Surprisingly, elements of the Microsoft Office suite are important: we see Excel appearing prominently as second most referenced word in preferred skills for these jobs. It is tied with a reference to "team", likely indicating being able to work on a team is highly valued along with the top preferred skill: communication. While a higher educational background may get one in the door, collaborative skills seem to be much more important. Certainly, we see references to analysis, software and (programming) languages but half of the top twenty refer to some kind of human-facing skill that we might not immediately think of in the wizardly world of data.

``` {r, eval=TRUE}
word_plot(job_preferences, jobpost_stopwords, 20, title = "Preferred Job Skills",
          fill_color = "salmon")
```

## Article Skill Lists

Another avenue was collecting skills described by online articles set to highlight the most important data science skills. In this case, 10 articles were reviewed and data was manually collected into a CSV file. 

### Data loading / Database Interlude II

Again, for ease of reproducibility, the data will be read from a CSV file. In the comments, however, would be how to load the data given the same connection as before. The rest of the process remains the same.

```{r, eval=TRUE, message=FALSE}
skills <- read_csv(vs_file, skip=2)
# skills <- read_db_tbl(connection, "SELECT * FROM valued_skills")
```

### Data wrangling

The data is manipulated into a workable dataframe below.

```{r, eval=TRUE}
a<-max(str_count(skills$Skill, "\\s"))
i=1
newCol <- c(1)
for(i in 1:(a+1)) {
  newCol[i] <- as.character(i)
}
newSkills <- skills %>% 
  separate(Skill, into = newCol, sep="\\s", fill="right")%>%
  gather(key="nCol", value="nSkill", 2:9) %>%
  mutate(nSkill = str_to_lower(nSkill)) %>%
  mutate(nSkill = str_replace(nSkill, "[ ,?:]", "") ) %>%
  filter(!is.na(nSkill)) %>%
  arrange(desc(nSkill)) 
# Machine and Learning are actually linked together after reviewing the csv file again
gskills <- newSkills %>%
  mutate(nSkill = if_else(nSkill %in% c("machine", "learning"), "machine learning", nSkill)) %>% 
  group_by(nSkill)%>%
  summarise(n=n())%>%
  arrange(desc(n))
gskills
```

### Results

The top five skills should be "Data" (including data analysis, data visualization, etc.), "Machine Learning", "Programming", "Communication" and "SQL".

```{r}
ggplot(data=(gskills[1:5,])) +
  geom_col(mapping = aes(x = fct_reorder(nSkill, n), y = n)) +
  coord_flip() +
  labs(title = "Most Referenced Skills", x = "Frequency", y = "Job Skills")
```

## Survey Data

# Data Acquisition

The data set used in this project is [Kaggle ML and Data Science Survey 2017](https://www.kaggle.com/kaggle/kaggle-survey-2017/feed). I downloaded the multiple choice item survey results in csv format and placed it in a GitHub repo (https://github.com/keshaws/CUNY_MSDS_2020/tree/master/DATA607)

## Importing Multiple Choice data

```{r message=FALSE, warning=FALSE }
surverydata_df <- read_csv (surverydata_link)
survey.data <- surverydata_df
#lets create a unique ID variable 
surverydata_df$id <- seq.int(nrow(surverydata_df))
dim(surverydata_df)
```

## Research Question

**Which are the most values data science skills?**

## Understanding Features 

Let's start gaining some insignts by exploring demographics features of the dataset.


```{r warning=FALSE, cache=FALSE, message=FALSE}
survey.demographics <- survey.data %>%
  select(GenderSelect,Country,Age,EmploymentStatus) %>%
  filter(Country!='NA',trimws(Country)!='',Age!='NA',trimws(GenderSelect) %in% c('Male','Female'))
# These workable results were stored to a database, and can be loaded given its restoration as follows:
# survey.demographics <- read_db_tbl(connection, "SELECT * FROM survey_demographics")
```




```{r warning=FALSE, cache=FALSE, message=FALSE}
survey.dem.age.plot <- survey.demographics %>%
    group_by(Age,GenderSelect) %>%
    summarise(count=n()) %>%
    arrange(desc(count))
survey.dem.plot <- survey.demographics %>%
  group_by(Age,Country,GenderSelect,EmploymentStatus) %>%
  summarise(count=n()) %>%
  arrange(desc(count))
```

```{r}
survey.dem.gen.plot <- survey.demographics %>%
    group_by(GenderSelect) %>%
    summarise(count=n()) %>%
    arrange(desc(count))
head(survey.dem.gen.plot)
```

Tidying the dataset and finding the percentile for each gender group

```{r}
survey.dem.tidy <- survey.dem.gen.plot %>%
    spread(GenderSelect,'count')
head(survey.dem.tidy)
```

```{r}
gender_percentile <- mutate(survey.dem.tidy,male_percent=round((Male/(Female+Male)*100),2),
                            female_percent=round((Female/(Female+Male))*100,2))
kable(gender_percentile)
```


```{r fig01,  echo=FALSE,warning=FALSE,fig.height = 10, fig.width = 8}
ggplot(data = survey.dem.plot, aes(x=GenderSelect,y=count))+
  geom_bar(stat = 'identity',aes(fill=GenderSelect))+
  facet_wrap(~EmploymentStatus, ncol = 2)+
  theme_bw()
``` 


There are 16716 survey respondents and we could see that there is a huge gender gap in the given dataset with over 83% are male and female makes up only ~17% of total. Also, most of respondents are full time employed followed by people who are not employed but looking for work. 

# EDA 

Let's take a look at data science activity attributes: TimeGatheringData, TimeModelBuilding, TimeProduction, TimeVisualizing, TimeFindingInsights.

## Data Science Activities

The US reponsdents data analysis show that gathering data is the main activitiy with higest time consumption 37.75%. The model building ranks 2nd, 19.23%, followed by time spent in finding insights and data visualization. Only 10.22% of total appears to be taken by prodcution activities. 


```{r fig02, echo=FALSE,warning=FALSE, cache=FALSE, message=FALSE}
survey.data.ds.activities <- survey.data %>%
    select(GenderSelect,Country,Age,EmploymentStatus,PublicDatasetsSelect,FormalEducation,MajorSelect,
           DataScienceIdentitySelect,CurrentJobTitleSelect,
          TimeGatheringData,TimeModelBuilding,TimeProduction,TimeVisualizing,TimeFindingInsights) %>%
          filter(Age!='NA',Country!='NA',Country!='',GenderSelect %in% c('Male', 'Female'),
          TimeGatheringData!='NA',TimeModelBuilding!='NA',TimeProduction!='NA',TimeVisualizing!='NA',
          TimeFindingInsights!='NA',MajorSelect!='',PublicDatasetsSelect!='NA',
          FormalEducation %in% c('Master\'s degree', 'Doctoral degree', 'Bachelor\'s degree')
          )
survey.data.ds.activities$dsid <- seq.int(nrow(survey.data.ds.activities))
ds.act.tidy <- survey.data.ds.activities %>%
          gather( DSActivity,act_count,TimeGatheringData:TimeFindingInsights) %>%
          arrange(dsid)
ds.act.df <- ds.act.tidy %>% select(dsid,Country,EmploymentStatus,DSActivity,act_count)
# Again, if ran from the restored SQL database, the following could be used instead:
# ds.act.df <- read_db_tbl(connection, "SELECT * FROM ds_activities")
ds.act.us <- filter(ds.act.df,Country =='United States')
ds.act.us.plot <- ds.act.us %>%
    group_by(DSActivity) %>%
    summarise(mean_precent=mean(act_count))
ds.act.us.plot.df <-  ds.act.us.plot %>%   
  arrange(desc(mean_precent))
kable(ds.act.us.plot.df)
```


```{r fig03, fig.height = 5, fig.width = 7, fig.align = "center" , message= FALSE, echo=FALSE}
ggplot(data = ds.act.us.plot,aes(DSActivity,mean_precent))+
  geom_bar(stat = 'identity',aes(fill=DSActivity))+  
  geom_text(aes(x = DSActivity, y = mean_precent, label = paste(round(mean_precent,2),'%'),
                                    group = DSActivity,vjust = -0.2))+
  labs(title = "Comparing Data Science Activities (Country:US)",x = "Data Science Activities", y = "Time Spent in %") +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 65, hjust = 1),legend.position = 'none')
```


## Learning platform 


```{r fig04, echo=FALSE,warning=FALSE, cache=FALSE, message=FALSE}
survey.data.learning <- survey.data %>%
    select(GenderSelect,Country,Age,EmploymentStatus,StudentStatus,LearningDataScience,CareerSwitcher,PublicDatasetsSelect,
           FormalEducation,MajorSelect,DataScienceIdentitySelect,CurrentJobTitleSelect,WorkChallengesSelect,
           LearningPlatformSelect,LearningPlatformUsefulnessArxiv,LearningPlatformUsefulnessBlogs,LearningPlatformUsefulnessCollege,
           LearningPlatformUsefulnessCompany,LearningPlatformUsefulnessConferences,LearningPlatformUsefulnessFriends,
           LearningPlatformUsefulnessKaggle,LearningPlatformUsefulnessNewsletters,LearningPlatformUsefulnessCommunities,
           LearningPlatformUsefulnessDocumentation,LearningPlatformUsefulnessCourses,LearningPlatformUsefulnessProjects,
           LearningPlatformUsefulnessPodcasts,LearningPlatformUsefulnessSO,LearningPlatformUsefulnessTextbook,
           LearningPlatformUsefulnessTradeBook,LearningPlatformUsefulnessTutoring,LearningPlatformUsefulnessYouTube,
           BlogsPodcastsNewslettersSelect,LearningDataScienceTime) %>%
    filter(Age!='NA',Country!='NA',Country!='',GenderSelect %in% c('Male', 'Female'))
survey.data.learning$lid <- seq.int(nrow(survey.data.learning))
survey.data.learning.tidy <- gather(survey.data.learning, LPlatform,LP_count,
                                    LearningPlatformUsefulnessArxiv:LearningPlatformUsefulnessYouTube) %>%
                          arrange(lid)
survey.data.learn.df  <- survey.data.learning.tidy %>%
    select(lid,Country,EmploymentStatus,LPlatform,LP_count) %>%
    group_by(LPlatform)
learn.df <- survey.data.learn.df %>%
    filter(LP_count!='NA', LP_count!='') %>% 
    ungroup()
# As before, the results of the manipulation above was stored on a table in the restorable database:
# learn.df <- read_db_tbl(connection, "SELECT * FROM learn_table")
ds.learn.us <- filter(learn.df, Country =='United States')
ds.learn.us.plot <- ds.learn.us %>%
    group_by(LPlatform) 
ds.learn.us.plot.df <- ds.learn.us.plot %>%
    mutate(LearningPlatform=substr(LPlatform,27,length(LPlatform))) 
kable(head(ds.learn.us.plot.df))
```


```{r fig05, fig.height = 7, fig.width = 8 , fig.align = "center" , message= FALSE, echo=FALSE}
ggplot(data = ds.learn.us.plot.df,aes(LearningPlatform,LP_count))+
  geom_bar(stat = 'identity',aes(fill=LP_count))+  
    labs(title = "Learning Platform Usage and Remarks (Country:US)", 
                         x = "Learning Platform Type", 
                         y = "Learners Sentiments") +
theme_bw()+
theme(axis.text.x = element_text(angle = 65, hjust = 1),axis.text.y=element_blank(),axis.ticks.y=element_blank())
```

The survery respondents used different learning platform and it appears that learners mostly benefited from personal projects as majority of resonse indicate it very useful. Online courses appears to be 2nd very useful only to be followed by StackOverflow and Kaggle. Blogs,textbooks and college also appear to be very userful whereas newsletters, podcasts and tradebook rank low. 

# Indeed.com

## Web Scrape Indeed.com for Data Scientist Job Vacancies

For our fourth approach, we pulled the text from the summary of job vacancies for data scientist positions on Indeed.com to get a sense of what skills are most in demand for data science. 

```{r}
indeed <- data.frame(summary=character(),stringsAsFactors=FALSE) 
for (i in seq(0, 990, 10)){
  url <- paste0('https://www.indeed.com/jobs?q=data+scientist&l=all&start=',i)
  file <- read_html(url)
  #summary
  summary <- file %>%
    html_nodes('#resultsCol .summary') %>%
    html_text() %>%
    str_extract(".+")
indeed <- rbind(indeed, as.data.frame(cbind(summary)))
}
```

## Transform Data

Change file format from a data frame into a text file that will work with TM.

```{r}
# data <- write.table(file, file = 'indeed.txt', sep = ",")
file <- "https://raw.githubusercontent.com/jambawilliams/DATA607Project3/master/indeed.txt"
text <- readLines(file)
docs <- Corpus(VectorSource(text))
```

## Condition Data

Remove spaces, non-skill words, and punctuation from extracted text.

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("data", "experience", "science", "predictive", "will", "including", "machine", "analytics", "technique", "advanced", "team", "understanding", "tools", "deep", "techniques", "decision", "work", "engineering", "forests", "modern", "trees", "glms", "years", "boosted...", "variety", "boosted", "seen", "software", "roles", "models", "closely", "statistical", "senior", "experts", "working", "engineer", "leverage", "learning", "scientific", "project", "domain", "scientists", "iquartic", "ocr", "use", "one", "etc", "insights", "scientist", "trends"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq=v)
d
```

## Visualize Data

By visualizing the relative frequency of skills listed in job summaries we can see which are more commonly mentioned in job postings online.  Python appears to be the number one skill in demand in this dataset.

```{r}
set.seed(1234)
wordcloud(words = d$word, freq=d$freq, min.freq = 50, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```
```{r}
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```

# Conclusion

Based on the different approaches taken by our team, we have seen a variety of different skill sets that are important for a data scientist to acquire and develop. While "soft-skills" like the ability to communicate clearly and navigate working in a collaborative environment are extremely in demand, they must be coupled with that "hard-skills" we are all familiar with: programming, data modeling, etc. In pursuit of this, there are a myriad of sources from which to draw from in becoming a more able practitioner of data science, though perhaps there is no better teacher than personal practice in the form of engaging personal projects and online learning.